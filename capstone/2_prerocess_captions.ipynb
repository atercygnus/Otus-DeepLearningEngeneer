{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = 'C:\\\\associative_represenations_data\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('tfeats.pkl', 'rb')\n",
    "tfeats = pickle.load(f)\n",
    "    \n",
    "f = open('vfeats.pkl', 'rb')\n",
    "vfeats = pickle.load(f)\n",
    "\n",
    "f = open('tcapts.pkl', 'rb')\n",
    "tcapts = pickle.load(f)\n",
    "    \n",
    "f = open('vcapts.pkl', 'rb')\n",
    "vcapts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118287, 2048)\n",
      "(5000, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(tfeats.shape)\n",
    "print(vfeats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118287, 5)\n",
      "(5000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(tcapts.shape)\n",
    "print(vcapts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['A man is in a funny position during a tennis match',\n",
       "        'A tennis player at the net after his play on the court. ',\n",
       "        'A man near the net playing tennis with official looking on.',\n",
       "        'a man by a tennis net getting ready to hit a ball',\n",
       "        'A man is attempting to return the ball'],\n",
       "       ['A white van is following an orange and white bus down the road. ',\n",
       "        'A van following behind a bus in the street. ',\n",
       "        'A white and orange bus driving down a city street.',\n",
       "        'A van follows behind a bus on a rural road.',\n",
       "        'A passenger bus that is driving down a street.'],\n",
       "       ['A group of children sitting around each other.',\n",
       "        'four children looking at each other one holding long object',\n",
       "        'A girl holding a tube talking to another girl.',\n",
       "        'Group of children sitting on a bench petting a dog.',\n",
       "        'The children are grouped together waiting their turn.'],\n",
       "       ...,\n",
       "       ['the elephants are  all next to each other under the tree',\n",
       "        'Several elephants standing in the shade of a tree.',\n",
       "        'Some very cute elephants in a grassy area.',\n",
       "        'Several elephants can be seen huddled together in the brush.',\n",
       "        'A few black elephants are out in the wild together. '],\n",
       "       ['A large industrial range in a kitchen with other small appliances.',\n",
       "        'a kitchen with a stove a coffee maker and a shelf',\n",
       "        'Large dirty stove in green and brown kitchen.',\n",
       "        'A kitchen with two ranges and an industrial coffee maker.  ',\n",
       "        'A kitchen area with stoves, shelves and a coffee maker.'],\n",
       "       ['Stuffed toy bears hanging on netting at indoor event.',\n",
       "        'Two teddy bears are attached to the net.',\n",
       "        'Teddy bears hanging from  a net near an HSBC sign.',\n",
       "        'Teddy bears home in front of an ice rink. ',\n",
       "        'A couple of brown teddy bears hanging from a metal fence.']],\n",
       "      dtype='<U250')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcapts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = \"#PAD#\"\n",
    "UNK = \"#UNK#\"\n",
    "START = \"#START#\"\n",
    "END = \"#END#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sentence into tokens (split into lowercased words)\n",
    "def split_sentence(sentence):\n",
    "    return list(filter(lambda x: len(x) > 0, re.split('\\W+', sentence.lower())))\n",
    "\n",
    "def generate_vocabulary(train_captions):\n",
    "\n",
    "    w_bag = [PAD, START, UNK, END] + [item.lower() for sublist in [split_sentence(sent) for sent in train_captions] for item in sublist]\n",
    "    vocab = Counter(w_bag)\n",
    "    vocab = [PAD, START, UNK, END] + [token for token, cnt in zip(vocab.keys(), vocab.values()) if cnt >= 5]\n",
    "    vocab = {key: i for i, key in enumerate(vocab)}\n",
    "\n",
    "    return vocab\n",
    "\n",
    "def caption_tokens_to_indices(captions, vocab):\n",
    "    return  [[vocab[START]] + [vocab[word] if word in vocab else vocab[UNK] for word in split_sentence(capt)] + [vocab[END]] for capt in captions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5154\n"
     ]
    }
   ],
   "source": [
    "# prepare vocabulary\n",
    "vocab = generate_vocabulary(tcapts[:, 3])\n",
    "vocab_inverse = {idx: w for w, idx in vocab.items()}\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use this during training\n",
    "def batch_captions_to_matrix(batch_captions, pad_idx, max_len=None):\n",
    "    \n",
    "    if max_len is None:\n",
    "        pad_len = max(map(len, batch_captions))\n",
    "    else:\n",
    "        pad_len = min(max(map(len, batch_captions)), max_len)\n",
    "        \n",
    "    matrix = []\n",
    "    for capt in batch_captions:\n",
    "        if pad_len-len(capt) >= 0:\n",
    "            matrix.append(np.pad(capt, (0, pad_len-len(capt)), mode='constant', constant_values=pad_idx))\n",
    "        else:\n",
    "            matrix.append(capt[:pad_len])\n",
    "    \n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_captions(out):\n",
    "    softmax = nn.Softmax(dim=1)(out).argmax(axis=1)\n",
    "    softmax_to_tokens = lambda batch: np.array(list(map(lambda token: vocab_inverse[token], batch.numpy().reshape(-1)))).reshape(*batch.shape)\n",
    "    return softmax_to_tokens(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = generate_vocabulary(tcapts[:, 3])\n",
    "tcapts_enc = batch_captions_to_matrix(caption_tokens_to_indices(tcapts[:, 3], vocab), vocab[PAD], max_len=50)\n",
    "vcapts_enc = batch_captions_to_matrix(caption_tokens_to_indices(vcapts[:, 3], vocab), vocab[PAD], max_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcapts_len = [len(x) for x in caption_tokens_to_indices(tcapts[:, 3], vocab)]\n",
    "vcapts_len = [len(x) for x in caption_tokens_to_indices(vcapts[:, 3], vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118287, 49)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcapts_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tcapts_encoded.pkl', 'wb') as file_embeds:\n",
    "    pickle.dump(tcapts_enc, file_embeds)\n",
    "    \n",
    "with open('vcapts_encoded.pkl', 'wb') as file_capts:\n",
    "    pickle.dump(vcapts_enc, file_capts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \n",
    "    IMG_EMBED_SIZE = tfeats.shape[1]\n",
    "    IMG_EMBED_BOTTLENECK = 120\n",
    "    WORD_EMBED_SIZE = 100\n",
    "    LSTM_UNITS = 300\n",
    "    LOGIT_BOTTLENECK = 120\n",
    "    pad_idx = vocab[PAD]\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        \n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        self.img_embed_to_bottleneck = nn.Linear(self.IMG_EMBED_SIZE, self.IMG_EMBED_BOTTLENECK)\n",
    "        self.img_embed_bottleneck_to_h0 = nn.Linear(self.IMG_EMBED_BOTTLENECK, nhid)\n",
    "        \n",
    "        self.embedding = nn.Embedding(ntoken, ninp)\n",
    "        \n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "            \n",
    "        self.token_logits_bottleneck = nn.Linear(nhid, self.LOGIT_BOTTLENECK)\n",
    "        self.token_logits = nn.Linear(self.LOGIT_BOTTLENECK, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        \n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.img_embed_to_bottleneck.bias.data.fill_(0)\n",
    "        self.img_embed_to_bottleneck.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.img_embed_bottleneck_to_h0.bias.data.fill_(0)\n",
    "        self.img_embed_bottleneck_to_h0.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.token_logits_bottleneck.bias.data.fill_(0)\n",
    "        self.token_logits_bottleneck.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.token_logits.bias.data.fill_(0)\n",
    "        self.token_logits.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, input_lengths, hidden=None):\n",
    "        hidden = self.img_embed_bottleneck_to_h0(self.img_embed_to_bottleneck(hidden))\n",
    "        \n",
    "        word_embeds = self.drop(self.embedding(x.T))\n",
    "        word_embeds = nn.utils.rnn.pack_padded_sequence(word_embeds, batch_len, enforce_sorted=False)\n",
    "        \n",
    "        output, hidden = self.rnn(word_embeds, (hidden, hidden))\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(output, total_length=x.shape[1])\n",
    "        output = self.drop(output)\n",
    "\n",
    "        output_bottleneck = self.token_logits_bottleneck(output.view(output.shape[0]*output.shape[1], output.shape[2]))\n",
    "        output = self.token_logits(output_bottleneck)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = RNNModel('LSTM', ntoken=len(vocab), ninp=RNNModel.WORD_EMBED_SIZE, nhid=RNNModel.LSTM_UNITS, nlayers=1, dropout=0.3).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "> <ipython-input-30-e9d94b289e29>(37)<module>()\n",
      "-> optimizer.step()\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  loss.grad\n",
      "(Pdb)  exit()\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-e9d94b289e29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-e9d94b289e29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;31m# None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'line'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'call'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "images_count = tcapts.shape[0]\n",
    "BATCH_SIZE = 32\n",
    "BATCH_COUNT = int(np.ceil(images_count/BATCH_SIZE))\n",
    "\n",
    "EVERY_BATCHES_TOSHOW = 100\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "decoder.train()\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "\n",
    "    print('EPOCH %d' % (e+1))\n",
    "\n",
    "    for i in range(BATCH_COUNT):\n",
    "        capts = tcapts_enc[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :]\n",
    "        target = torch.Tensor(np.hstack((capts, np.array([vocab[PAD]]*capts.shape[0]).reshape(-1, 1)))[:, 1:]).long()\n",
    "        im_feats = tfeats[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :]\n",
    "\n",
    "        batch_len = tcapts_len[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "\n",
    "        capts = torch.Tensor(capts).long()\n",
    "        im_feats = torch.Tensor(np.expand_dims(im_feats, 0))\n",
    "\n",
    "        decoder.zero_grad()\n",
    "\n",
    "        out, hidden = decoder(capts.cuda(), batch_len, im_feats.cuda())\n",
    "        loss = criterion(out, target.view(-1).cuda())\n",
    "\n",
    "        if i > 0 and i % EVERY_BATCHES_TOSHOW == 0:\n",
    "            print(\"Batch {}: loss: {}\".format(i, loss.cpu().detach().numpy()))\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        pdb.set_trace()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(tcapts_enc[:30, :]).long().cuda()\n",
    "batch_len = tcapts_len[:30]\n",
    "hidden = torch.Tensor(np.expand_dims(tfeats[:30, :], 0)).cuda()\n",
    "\n",
    "decoder.eval()\n",
    "\n",
    "out, hid = decoder(x, batch_len, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'#PAD#': 1470})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(decode_captions(out.detach().cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([[vocab[START]]*5]).long()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2048])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = np.expand_dims(tfeats[0], 0)\n",
    "hidden = np.expand_dims(hidden, 0)\n",
    "hidden = torch.tensor(hidden)\n",
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "tabulate: failed to synchronize: cudaErrorAssert: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-331931e0271c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-4fcd68190fc2>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, input_lengths, hidden)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mword_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mword_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_embeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_embeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\utils\\rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[1;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\utils\\rnn.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(cls, data, batch_sizes, sorted_indices, unsorted_indices)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# should only be used internally.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0munsorted_indices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0munsorted_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minvert_permutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;31m# support being called as `PackedSequence(data, batch_sizes, sorted_indices)`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\utils\\rnn.py\u001b[0m in \u001b[0;36minvert_permutation\u001b[1;34m(permutation)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     output.scatter_(0, permutation,\n\u001b[1;32m--> 230\u001b[1;33m                     torch.arange(0, permutation.numel(), device=permutation.device))\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: tabulate: failed to synchronize: cudaErrorAssert: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "decoder(x.cuda(), [1], hidden.cuda())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
