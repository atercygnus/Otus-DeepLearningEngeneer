{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext import data\n",
    "\n",
    "from torchtext.datasets import LanguageModelingDataset\n",
    "\n",
    "import os\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math \n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "import wiki_utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Задание: Используя подход аналогичный torchvision, сделать свой класс датасета.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возмём <a href=\"rnn.ipynb\">задание</a> из урока, и переделаем его под датасет torchtext. Возьмём [отсюда](https://pytorch.org/text/_modules/torchtext/datasets/language_modeling.html) код датасета WikiText-2, и переделаем так, чтобы он взял наши данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wikitext(LanguageModelingDataset):\n",
    "\n",
    "    urls = []\n",
    "    name = 'wikitext'\n",
    "    dirname = '../data/wikitext'\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, root='wikitext', train='train.txt',\n",
    "               validation='valid.txt', test='test.txt',\n",
    "               **kwargs):\n",
    "        \"\"\"Create dataset objects for splits of the wikitext dataset.\n",
    "        This is the most flexible way to use the dataset.\n",
    "        Arguments:\n",
    "            text_field: The field that will be used for text data.\n",
    "            root: The root directory that the dataset's zip archive will be\n",
    "                expanded into; therefore the directory in whose wikitext\n",
    "                subdirectory the data files will be stored.\n",
    "            train: The filename of the train data. Default: 'wiki.train.tokens'.\n",
    "            validation: The filename of the validation data, or None to not\n",
    "                load the validation set. Default: 'wiki.valid.tokens'.\n",
    "            test: The filename of the test data, or None to not load the test\n",
    "                set. Default: 'wiki.test.tokens'.\n",
    "        \"\"\"\n",
    "        return super(wikitext, cls).splits(\n",
    "            root=root, train=train, validation=validation, test=test,\n",
    "            text_field=text_field, **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def iters(cls, batch_size=32, bptt_len=35, device=0, root='.data',\n",
    "              vectors=None, **kwargs):\n",
    "        \"\"\"Create iterator objects for splits of the wikitext dataset.\n",
    "        This is the simplest way to use the dataset, and assumes common\n",
    "        defaults for field, vocabulary, and iterator parameters.\n",
    "        Arguments:\n",
    "            batch_size: Batch size.\n",
    "            bptt_len: Length of sequences for backpropagation through time.\n",
    "            device: Device to create batches on. Use -1 for CPU and None for\n",
    "                the currently active GPU device.\n",
    "            root: The root directory that the dataset's zip archive will be\n",
    "                expanded into; therefore the directory in whose wikitext\n",
    "                subdirectory the data files will be stored.\n",
    "            wv_dir, wv_type, wv_dim: Passed to the Vocab constructor for the\n",
    "                text field. The word vectors are accessible as\n",
    "                train.dataset.fields['text'].vocab.vectors.\n",
    "            Remaining keyword arguments: Passed to the splits method.\n",
    "        \"\"\"\n",
    "        TEXT = data.Field()\n",
    "\n",
    "        train, val, test = cls.splits(TEXT, root=root, **kwargs)\n",
    "\n",
    "        TEXT.build_vocab(train, vectors=vectors)\n",
    "\n",
    "        return data.BPTTIterator.splits(\n",
    "            (train, val, test), batch_size=batch_size, bptt_len=bptt_len,\n",
    "            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "eval_batch_size = 128\n",
    "sequence_length = 30\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=lambda x: list(x))\n",
    "\n",
    "train, valid, test = wikitext.splits(TEXT, root='../')\n",
    "\n",
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size=batch_size,\n",
    "    bptt_len=sequence_length,\n",
    "    device='cuda',\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(TEXT.vocab)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i, (data, targets) in enumerate(data_loader):\n",
    "        output, hidden = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(TEXT.vocab)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        output, hidden = model(batch.text.cuda())\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += criterion(output_flat, batch.target.view(-1).cuda()).item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ntokens = len(TEXT.vocab)\n",
    "    #for i, (data, targets) in enumerate(train_iter):\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(batch.text.cuda())\n",
    "        loss = criterion(output.view(-1, ntokens), batch.target.view(-1).cuda())\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, i, len(train_iter), lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab)\n",
    "model = RNNModel('LSTM', ntokens, 128, 128, 2, 0.3).cuda()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n=50, temp=1.):\n",
    "    model.eval()\n",
    "    x = torch.rand(1, 1).mul(ntokens).long().cuda()\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "        s = TEXT.vocab.itos[s_idx]\n",
    "        out.append(s)\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " ‘çdaş„ッ動्đėễ²キ⅓ხ—ư×♯o्ầô€اsż ʿ^?)ăñアăن⁄üα€āリв\\ăł⅔” \n",
      "\n",
      "| epoch   1 |   100/ 2808 batches | lr 4.00 | loss  3.46 | ppl    31.79\n",
      "| epoch   1 |   200/ 2808 batches | lr 4.00 | loss  3.15 | ppl    23.45\n",
      "| epoch   1 |   300/ 2808 batches | lr 4.00 | loss  3.13 | ppl    22.77\n",
      "| epoch   1 |   400/ 2808 batches | lr 4.00 | loss  3.10 | ppl    22.27\n",
      "| epoch   1 |   500/ 2808 batches | lr 4.00 | loss  3.10 | ppl    22.29\n",
      "| epoch   1 |   600/ 2808 batches | lr 4.00 | loss  3.09 | ppl    21.95\n",
      "| epoch   1 |   700/ 2808 batches | lr 4.00 | loss  2.99 | ppl    19.90\n",
      "| epoch   1 |   800/ 2808 batches | lr 4.00 | loss  2.85 | ppl    17.21\n",
      "| epoch   1 |   900/ 2808 batches | lr 4.00 | loss  2.73 | ppl    15.37\n",
      "| epoch   1 |  1000/ 2808 batches | lr 4.00 | loss  2.63 | ppl    13.81\n",
      "| epoch   1 |  1100/ 2808 batches | lr 4.00 | loss  2.52 | ppl    12.47\n",
      "| epoch   1 |  1200/ 2808 batches | lr 4.00 | loss  2.47 | ppl    11.81\n",
      "| epoch   1 |  1300/ 2808 batches | lr 4.00 | loss  2.43 | ppl    11.31\n",
      "| epoch   1 |  1400/ 2808 batches | lr 4.00 | loss  2.37 | ppl    10.74\n",
      "| epoch   1 |  1500/ 2808 batches | lr 4.00 | loss  2.34 | ppl    10.39\n",
      "| epoch   1 |  1600/ 2808 batches | lr 4.00 | loss  2.31 | ppl    10.05\n",
      "| epoch   1 |  1700/ 2808 batches | lr 4.00 | loss  2.28 | ppl     9.74\n",
      "| epoch   1 |  1800/ 2808 batches | lr 4.00 | loss  2.25 | ppl     9.48\n",
      "| epoch   1 |  1900/ 2808 batches | lr 4.00 | loss  2.22 | ppl     9.25\n",
      "| epoch   1 |  2000/ 2808 batches | lr 4.00 | loss  2.19 | ppl     8.97\n",
      "| epoch   1 |  2100/ 2808 batches | lr 4.00 | loss  2.18 | ppl     8.80\n",
      "| epoch   1 |  2200/ 2808 batches | lr 4.00 | loss  2.15 | ppl     8.60\n",
      "| epoch   1 |  2300/ 2808 batches | lr 4.00 | loss  2.14 | ppl     8.51\n",
      "| epoch   1 |  2400/ 2808 batches | lr 4.00 | loss  2.12 | ppl     8.30\n",
      "| epoch   1 |  2500/ 2808 batches | lr 4.00 | loss  2.10 | ppl     8.19\n",
      "| epoch   1 |  2600/ 2808 batches | lr 4.00 | loss  2.09 | ppl     8.05\n",
      "| epoch   1 |  2700/ 2808 batches | lr 4.00 | loss  2.07 | ppl     7.92\n",
      "| epoch   1 |  2800/ 2808 batches | lr 4.00 | loss  2.04 | ppl     7.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  1.87 | valid ppl     6.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  to @-@ ansheme of depxepvio hass . the filyic iol \n",
      "\n",
      "| epoch   2 |   100/ 2808 batches | lr 4.00 | loss  2.05 | ppl     7.80\n",
      "| epoch   2 |   200/ 2808 batches | lr 4.00 | loss  2.02 | ppl     7.50\n",
      "| epoch   2 |   300/ 2808 batches | lr 4.00 | loss  2.01 | ppl     7.45\n",
      "| epoch   2 |   400/ 2808 batches | lr 4.00 | loss  2.00 | ppl     7.36\n",
      "| epoch   2 |   500/ 2808 batches | lr 4.00 | loss  1.98 | ppl     7.25\n",
      "| epoch   2 |   600/ 2808 batches | lr 4.00 | loss  1.97 | ppl     7.18\n",
      "| epoch   2 |   700/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.12\n",
      "| epoch   2 |   800/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.06\n",
      "| epoch   2 |   900/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.02\n",
      "| epoch   2 |  1000/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.96\n",
      "| epoch   2 |  1100/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.85\n",
      "| epoch   2 |  1200/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.82\n",
      "| epoch   2 |  1300/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.75\n",
      "| epoch   2 |  1400/ 2808 batches | lr 4.00 | loss  1.89 | ppl     6.61\n",
      "| epoch   2 |  1500/ 2808 batches | lr 4.00 | loss  1.89 | ppl     6.64\n",
      "| epoch   2 |  1600/ 2808 batches | lr 4.00 | loss  1.89 | ppl     6.60\n",
      "| epoch   2 |  1700/ 2808 batches | lr 4.00 | loss  1.88 | ppl     6.54\n",
      "| epoch   2 |  1800/ 2808 batches | lr 4.00 | loss  1.87 | ppl     6.52\n",
      "| epoch   2 |  1900/ 2808 batches | lr 4.00 | loss  1.88 | ppl     6.52\n",
      "| epoch   2 |  2000/ 2808 batches | lr 4.00 | loss  1.86 | ppl     6.43\n",
      "| epoch   2 |  2100/ 2808 batches | lr 4.00 | loss  1.86 | ppl     6.43\n",
      "| epoch   2 |  2200/ 2808 batches | lr 4.00 | loss  1.85 | ppl     6.37\n",
      "| epoch   2 |  2300/ 2808 batches | lr 4.00 | loss  1.85 | ppl     6.38\n",
      "| epoch   2 |  2400/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.27\n",
      "| epoch   2 |  2500/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.26\n",
      "| epoch   2 |  2600/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.28\n",
      "| epoch   2 |  2700/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.21\n",
      "| epoch   2 |  2800/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  1.61 | valid ppl     5.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  abert the sumpery . <eos> <eos> <eos> = = regiined : hwinaris \n",
      "\n",
      "| epoch   3 |   100/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.25\n",
      "| epoch   3 |   200/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.08\n",
      "| epoch   3 |   300/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.08\n",
      "| epoch   3 |   400/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.05\n",
      "| epoch   3 |   500/ 2808 batches | lr 4.00 | loss  1.79 | ppl     6.02\n",
      "| epoch   3 |   600/ 2808 batches | lr 4.00 | loss  1.79 | ppl     5.99\n",
      "| epoch   3 |   700/ 2808 batches | lr 4.00 | loss  1.79 | ppl     5.99\n",
      "| epoch   3 |   800/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.95\n",
      "| epoch   3 |   900/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.95\n",
      "| epoch   3 |  1000/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.94\n",
      "| epoch   3 |  1100/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
      "| epoch   3 |  1200/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.90\n",
      "| epoch   3 |  1300/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.87\n",
      "| epoch   3 |  1400/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.76\n",
      "| epoch   3 |  1500/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.80\n",
      "| epoch   3 |  1600/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.79\n",
      "| epoch   3 |  1700/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.75\n",
      "| epoch   3 |  1800/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.77\n",
      "| epoch   3 |  1900/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.82\n",
      "| epoch   3 |  2000/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
      "| epoch   3 |  2100/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.77\n",
      "| epoch   3 |  2200/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.76\n",
      "| epoch   3 |  2300/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.75\n",
      "| epoch   3 |  2400/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
      "| epoch   3 |  2500/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
      "| epoch   3 |  2600/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.71\n",
      "| epoch   3 |  2700/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
      "| epoch   3 |  2800/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  1.52 | valid ppl     4.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  the save was station much cention deferon to find \n",
      "\n",
      "| epoch   4 |   100/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
      "| epoch   4 |   200/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   4 |   300/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch   4 |   400/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch   4 |   500/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   4 |   600/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.56\n",
      "| epoch   4 |   700/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   4 |   800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   4 |   900/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch   4 |  1000/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   4 |  1100/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
      "| epoch   4 |  1200/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   4 |  1300/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
      "| epoch   4 |  1400/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.42\n",
      "| epoch   4 |  1500/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "| epoch   4 |  1600/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.47\n",
      "| epoch   4 |  1700/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.44\n",
      "| epoch   4 |  1800/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "| epoch   4 |  1900/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.50\n",
      "| epoch   4 |  2000/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "| epoch   4 |  2100/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.49\n",
      "| epoch   4 |  2200/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "| epoch   4 |  2300/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "| epoch   4 |  2400/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.42\n",
      "| epoch   4 |  2500/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.42\n",
      "| epoch   4 |  2600/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "| epoch   4 |  2700/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.43\n",
      "| epoch   4 |  2800/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  1.48 | valid ppl     4.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  at the 2007 . a stablich alongs . their presport  \n",
      "\n",
      "| epoch   5 |   100/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "| epoch   5 |   200/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.36\n",
      "| epoch   5 |   300/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.39\n",
      "| epoch   5 |   400/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.39\n",
      "| epoch   5 |   500/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.37\n",
      "| epoch   5 |   600/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.35\n",
      "| epoch   5 |   700/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.37\n",
      "| epoch   5 |   800/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.35\n",
      "| epoch   5 |   900/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.35\n",
      "| epoch   5 |  1000/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.35\n",
      "| epoch   5 |  1100/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.32\n",
      "| epoch   5 |  1200/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.35\n",
      "| epoch   5 |  1300/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.32\n",
      "| epoch   5 |  1400/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.25\n",
      "| epoch   5 |  1500/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.29\n",
      "| epoch   5 |  1600/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.30\n",
      "| epoch   5 |  1700/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.26\n",
      "| epoch   5 |  1800/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.28\n",
      "| epoch   5 |  1900/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.35\n",
      "| epoch   5 |  2000/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.28\n",
      "| epoch   5 |  2100/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.33\n",
      "| epoch   5 |  2200/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.31\n",
      "| epoch   5 |  2300/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.32\n",
      "| epoch   5 |  2400/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.26\n",
      "| epoch   5 |  2500/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.26\n",
      "| epoch   5 |  2600/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.31\n",
      "| epoch   5 |  2700/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.28\n",
      "| epoch   5 |  2800/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  1.45 | valid ppl     4.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ican commanders for per very in then opened work t \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(50), '\\n')\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train()\n",
    "    val_loss = evaluate(valid_iter)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(50), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
