{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "\n",
    "### Д/з из четырех пунктов:\n",
    "* Улучшение `fit_generator`\n",
    "* Сравнение двух ReLU (разные активации)\n",
    "* Испорченный батч-норм \n",
    "* \"Сырые\" данные. \n",
    "\n",
    "### Что нужно сделать\n",
    "* Следовать инструкциям в каждом из пунктов.\n",
    "* Результатами вашей работы будет ноутбук с доработанным кодом + архив с директорией с логами `tensorboard` `logs/`, в который вы запишите результаты экспериментов. Подробности в инструкциях ниже.\n",
    "* Можно и нужно пользоваться кодом из файла `utils`, **но** весь код модифицируйте, пожалуйста, в ноутбуках! Так мне будет проще проверять."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Загрузка tensorboard в ноутбук**\n",
    "\n",
    "Можете попробовать использовать его так на свой страх и риск :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Импорты**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт слоев для д/з"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import BatchNormFlawed, Dense, DenseSmart, Sequential, MNISTSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных\n",
    "\n",
    "> Здесь ничего менять не нужно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_tr, y_tr), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = MNISTSequence(X_tr, y_tr, 128)\n",
    "test_seq = MNISTSequence(X_test, y_test, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Очистка данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Улучшение fit_generator\n",
    "\n",
    "Улучшите метод `fit_generator` так, чтобы он:\n",
    "* Записывал значения градиентов для всех переменных при помощи `tf.summary.histogram` \n",
    "* Записывал значения ошибки и метрики на валидации с помощью `tf.summary.scalar`\n",
    "\n",
    "Затем сделайте monkey patch класса sequential обновленным методом (следующая ячейка за методом `fit_generator`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_generator(self, train_seq, eval_seq, epoch, loss, optimizer, writer=None):\n",
    "    history = dict(train=list(), val=list())\n",
    "\n",
    "    train_loss_results = list()\n",
    "    val_loss_results = list()\n",
    "\n",
    "    train_accuracy_results = list()\n",
    "    val_accuracy_results = list()\n",
    "\n",
    "    step = 0\n",
    "    for e in range(epoch):\n",
    "        p = tf.keras.metrics.Mean()\n",
    "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        epoch_loss_avg_val = tf.keras.metrics.Mean()\n",
    "\n",
    "        epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        epoch_accuracy_val = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "        for x, y in train_seq:\n",
    "            with tf.GradientTape() as tape:\n",
    "                \"\"\"\n",
    "                Обратите внимание! Если записывать гистограмму каждый шаг,\n",
    "                обучение будет идти очень медленно. Поэтому записываем данные \n",
    "                каждый i-й шаг.\n",
    "                \"\"\"\n",
    "                if step % 50 == 0:\n",
    "                    prediction = self._forward(x, writer, step)\n",
    "                else:\n",
    "                    prediction = self._forward(x)\n",
    "                loss_value = loss(y, prediction)\n",
    "                     \n",
    "            ###############################################################\n",
    "            #                                                             #\n",
    "            # Добавьте запись градиентов в гистограммы                    #\n",
    "            #                                                             #\n",
    "            ###############################################################\n",
    "        \n",
    "            gradients = tape.gradient(loss_value, self._trainable_variables)\n",
    "            \n",
    "            if step % 50 == 0:\n",
    "                \"\"\"\n",
    "                Пример того, как можно дать всем градиентам уникальные имена. \n",
    "                Обратите внимание! Создание grad_names лучше вынести из цикла,\n",
    "                чтобы не пересоздавать список на каждом шаге! \n",
    "                \"\"\"\n",
    "                grad_names = list()\n",
    "                for layer in self._layers:\n",
    "                    for var_num, var in enumerate(layer.get_trainable()):\n",
    "                        grad_names.append(f\"grad_{layer.name}_{var_num}\")\n",
    "\n",
    "                if writer is not None:\n",
    "                    with writer.as_default():\n",
    "                        for i, gradient in enumerate(gradients):\n",
    "                            tf.summary.histogram(grad_names[i], gradients[i], step=step)\n",
    "            \n",
    "            optimizer.apply_gradients(zip(gradients, self._trainable_variables))\n",
    "            epoch_accuracy.update_state(y, prediction)\n",
    "            epoch_loss_avg.update_state(loss_value)\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                with writer.as_default():\n",
    "                    tf.summary.scalar('train_accuracy', epoch_accuracy.result().numpy(), step=step)\n",
    "                    tf.summary.scalar('train_loss', epoch_loss_avg.result().numpy(), step=step)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        train_accuracy_results.append(epoch_accuracy.result().numpy())\n",
    "        train_loss_results.append(epoch_loss_avg.result().numpy())\n",
    "\n",
    "        for valbatch_n, (x, y) in enumerate(eval_seq):\n",
    "            prediction = self._forward(x)\n",
    "            loss_value = loss(y, prediction)\n",
    "            epoch_loss_avg_val.update_state(loss_value)\n",
    "            epoch_accuracy_val.update_state(y, prediction)\n",
    "     \n",
    "            ###############################################################\n",
    "            #                                                             #\n",
    "            # Добавьте сохранение метрики и функции ошибки на валидации   #\n",
    "            #                                                             #\n",
    "            ###############################################################\n",
    "            \n",
    "        # Не вижу смысла считать качество валидации на каких-то отдельных батчах, \n",
    "        # запишем качество валидации на всей тестовой выборке в конце каждой эпохи\n",
    "        if writer is not None:\n",
    "            with writer.as_default():\n",
    "                tf.summary.scalar('val_accuracy', epoch_accuracy_val.result().numpy(), step=e*len(train_seq))\n",
    "                tf.summary.scalar('val_loss', epoch_loss_avg_val.result().numpy(), step=e*len(train_seq))\n",
    "            \n",
    "        val_accuracy_results.append(epoch_accuracy_val.result().numpy())\n",
    "        val_loss_results.append(epoch_loss_avg_val.result().numpy())\n",
    "\n",
    "        print(\"Epoch {}: Train loss: {:.3f} Train Accuracy: {:.3f}\".format(e + 1,\n",
    "                                                                           train_loss_results[-1],\n",
    "                                                                           train_accuracy_results[-1]))\n",
    "        print(\"Epoch {}: Val loss: {:.3f} Val Accuracy: {:.3f}\".format(e + 1,\n",
    "                                                                       val_loss_results[-1],\n",
    "                                                                       val_accuracy_results[-1]))\n",
    "        print('*' * 20)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monkey patch: обновляем метод\n",
    "Sequential.fit_generator = fit_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Сравнение двух ReLU (разные активации)\n",
    "\n",
    "Запустите два эксперимента ниже. Сравните результаты - значения метрик после каждого из них.\n",
    "\n",
    "Запустите tensorboard, изучите распределения активаций, градиентов и т.д. для `relu` и `smart_dense_relu`. \n",
    "\n",
    "Подумайте, почему в одном случае сеть обучается плохо, а в другом - хорошо. Вставьте в ноутбук (или напишите список названий) тех графики из tensorboard, которые, по вашему мнению, это иллюстрируют, и напишите, почему.\n",
    "\n",
    "\n",
    "Команда для запуска tensorboard в bash:\n",
    "\n",
    "`$ tensorboard --logdir logs/`\n",
    "\n",
    "**Ваш комментарий:**\n",
    "\n",
    "<span style=\"color:red \">__На мой взгляд, больше всего информации дают графики с градиентами.__</span>\n",
    "\n",
    "<img src=\"img/relu_vs_smart_dense_relu.png\" style=\"width:70%\">\n",
    "\n",
    "<span style=\"color:red \">__У слоёв с правильной инициализацией мы видим огромный градиент в самом начале, чего не наблюдается у слоя с обчной инициализацией. Это означает, что начальное положение весов находится где-то на грани \"обрыва\", и алгоритм быстро находит хороший минимум.__</span>\n",
    "\n",
    "<span style=\"color:red \">__В дальшейшем градиенты у слоя с правильной инициализацией в среднем гораздо меньше, чем у слоя с обычной инициализацией. Неудивительно, ведь мы уже в хорошем минимуме.__</span>\n",
    "\n",
    "<span style=\"color:red \">__Однако, не все градиенты ведут себя подобным образом. Исключение по непонятной мне причине составляет bias второго полносвязного слоя (внимание на масштаб). В то время, как градиенты слоя с обыной инициализацией как правило не превышают 2, градиенты слоя с правильной инициализацией в несколько раз больше.__</span>\n",
    "\n",
    "<img src=\"img/relu_vs_smart_dense_relu_2.png\" style=\"width:70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 11.731 Train Accuracy: 0.269\n",
      "Epoch 1: Val loss: 9.938 Val Accuracy: 0.379\n",
      "********************\n",
      "Epoch 2: Train loss: 8.771 Train Accuracy: 0.452\n",
      "Epoch 2: Val loss: 7.943 Val Accuracy: 0.504\n",
      "********************\n",
      "Epoch 3: Train loss: 7.674 Train Accuracy: 0.521\n",
      "Epoch 3: Val loss: 7.131 Val Accuracy: 0.555\n",
      "********************\n",
      "Epoch 4: Train loss: 7.089 Train Accuracy: 0.557\n",
      "Epoch 4: Val loss: 6.743 Val Accuracy: 0.579\n",
      "********************\n",
      "Epoch 5: Train loss: 6.719 Train Accuracy: 0.580\n",
      "Epoch 5: Val loss: 6.434 Val Accuracy: 0.598\n",
      "********************\n",
      "Epoch 6: Train loss: 6.470 Train Accuracy: 0.596\n",
      "Epoch 6: Val loss: 6.282 Val Accuracy: 0.608\n",
      "********************\n",
      "Epoch 7: Train loss: 6.311 Train Accuracy: 0.607\n",
      "Epoch 7: Val loss: 6.152 Val Accuracy: 0.616\n",
      "********************\n",
      "Epoch 8: Train loss: 6.203 Train Accuracy: 0.613\n",
      "Epoch 8: Val loss: 6.063 Val Accuracy: 0.622\n",
      "********************\n",
      "Epoch 9: Train loss: 6.121 Train Accuracy: 0.618\n",
      "Epoch 9: Val loss: 6.022 Val Accuracy: 0.625\n",
      "********************\n",
      "Epoch 10: Train loss: 5.578 Train Accuracy: 0.651\n",
      "Epoch 10: Val loss: 4.809 Val Accuracy: 0.699\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "writer = tf.summary.create_file_writer(\"logs/relu\")\n",
    "\n",
    "model = Sequential(Dense(784, 100, tf.nn.relu, 'dense1'), \n",
    "                   Dense(100, 100, tf.nn.relu, 'dense2'), \n",
    "                   Dense(100, 10, tf.nn.softmax, 'output'))\n",
    "\n",
    "hist = model.fit_generator(train_seq, test_seq, 10,\n",
    "                           keras.losses.sparse_categorical_crossentropy, \n",
    "                           keras.optimizers.Adam(),\n",
    "                           writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 0.323 Train Accuracy: 0.904\n",
      "Epoch 1: Val loss: 0.183 Val Accuracy: 0.945\n",
      "********************\n",
      "Epoch 2: Train loss: 0.129 Train Accuracy: 0.962\n",
      "Epoch 2: Val loss: 0.136 Val Accuracy: 0.960\n",
      "********************\n",
      "Epoch 3: Train loss: 0.086 Train Accuracy: 0.975\n",
      "Epoch 3: Val loss: 0.122 Val Accuracy: 0.964\n",
      "********************\n",
      "Epoch 4: Train loss: 0.064 Train Accuracy: 0.981\n",
      "Epoch 4: Val loss: 0.110 Val Accuracy: 0.969\n",
      "********************\n",
      "Epoch 5: Train loss: 0.047 Train Accuracy: 0.985\n",
      "Epoch 5: Val loss: 0.133 Val Accuracy: 0.962\n",
      "********************\n",
      "Epoch 6: Train loss: 0.038 Train Accuracy: 0.988\n",
      "Epoch 6: Val loss: 0.148 Val Accuracy: 0.958\n",
      "********************\n",
      "Epoch 7: Train loss: 0.031 Train Accuracy: 0.991\n",
      "Epoch 7: Val loss: 0.144 Val Accuracy: 0.961\n",
      "********************\n",
      "Epoch 8: Train loss: 0.028 Train Accuracy: 0.991\n",
      "Epoch 8: Val loss: 0.125 Val Accuracy: 0.967\n",
      "********************\n",
      "Epoch 9: Train loss: 0.025 Train Accuracy: 0.992\n",
      "Epoch 9: Val loss: 0.134 Val Accuracy: 0.967\n",
      "********************\n",
      "Epoch 10: Train loss: 0.024 Train Accuracy: 0.992\n",
      "Epoch 10: Val loss: 0.137 Val Accuracy: 0.967\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "writer = tf.summary.create_file_writer(\"logs/relu_smart_dense\")\n",
    "\n",
    "model = Sequential(DenseSmart(784, 100, tf.nn.relu, 'dense1'), \n",
    "                   DenseSmart(100, 100, tf.nn.relu, 'dense2'), \n",
    "                   DenseSmart(100, 10, tf.nn.softmax, 'output'))\n",
    "\n",
    "hist = model.fit_generator(train_seq, test_seq, 10,\n",
    "                           keras.losses.sparse_categorical_crossentropy, \n",
    "                           keras.optimizers.Adam(),\n",
    "                           writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.a Испорченный батч-норм \n",
    "\n",
    "Запустите два эксперимент ниже. \n",
    "\n",
    "Почему обучение не идет? В чем ошибка в слое `BatchNorm`? Изучите и исправьте код метода `__call__` (Шаблон находится ниже под блоком с экспериментом.).\n",
    "\n",
    "Можно пользоваться tensorboard, если он нужен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU + Batch Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 2.528 Train Accuracy: 0.111\n",
      "Epoch 1: Val loss: 2.301 Val Accuracy: 0.113\n",
      "********************\n",
      "Epoch 2: Train loss: 2.301 Train Accuracy: 0.112\n",
      "Epoch 2: Val loss: 2.301 Val Accuracy: 0.113\n",
      "********************\n",
      "Epoch 3: Train loss: 2.301 Train Accuracy: 0.112\n",
      "Epoch 3: Val loss: 2.301 Val Accuracy: 0.113\n",
      "********************\n",
      "Epoch 4: Train loss: 2.301 Train Accuracy: 0.112\n",
      "Epoch 4: Val loss: 2.301 Val Accuracy: 0.113\n",
      "********************\n",
      "Epoch 5: Train loss: 2.301 Train Accuracy: 0.112\n",
      "Epoch 5: Val loss: 2.301 Val Accuracy: 0.113\n",
      "********************\n",
      "Epoch 6: Train loss: 2.301 Train Accuracy: 0.112\n",
      "Epoch 6: Val loss: 2.301 Val Accuracy: 0.113\n",
      "********************\n",
      "Epoch 7: Train loss: 2.301 Train Accuracy: 0.112\n",
      "Epoch 7: Val loss: 2.301 Val Accuracy: 0.113\n",
      "********************\n",
      "Epoch 8: Train loss: 2.301 Train Accuracy: 0.112\n",
      "Epoch 8: Val loss: 2.301 Val Accuracy: 0.113\n",
      "********************\n",
      "Epoch 9: Train loss: 2.301 Train Accuracy: 0.112\n",
      "Epoch 9: Val loss: 2.301 Val Accuracy: 0.113\n",
      "********************\n",
      "Epoch 10: Train loss: 2.301 Train Accuracy: 0.112\n",
      "Epoch 10: Val loss: 2.301 Val Accuracy: 0.113\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "writer = tf.summary.create_file_writer(\"logs/relu_bn\")\n",
    "\n",
    "model = Sequential(Dense(784, 100, tf.nn.relu, 'dense'), \n",
    "                   BatchNormFlawed('batch_norm'), \n",
    "                   Dense(100, 100, tf.nn.relu, 'dense1'), \n",
    "                   Dense(100, 10, tf.nn.softmax, 'dense2'))\n",
    "\n",
    "hist = model.fit_generator(train_seq, test_seq, 10,\n",
    "                           keras.losses.sparse_categorical_crossentropy, \n",
    "                           keras.optimizers.Adam(),\n",
    "                           writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Класс, который нужно исправить**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormFixed(BatchNormFlawed):\n",
    "    def __call__(self, x, writer=None, step=None):\n",
    "        \"\"\"\n",
    "        Исправьте блок кода ниже так, чтобы модель обучалась, не появлялись значения loss = NaN        \"\"\"\n",
    "        mu = tf.reduce_mean(x, axis=0)\n",
    "        sigma = tf.dtypes.cast(tf.math.reduce_std(x, axis=0), tf.double)\n",
    "        normed = (x - mu) / tf.add(sigma, tf.constant(0.001, dtype=tf.double))\n",
    "        out = normed * self._gamma + self._beta\n",
    "        \"\"\"\n",
    "        Конец блока, который нужно исправить\n",
    "        \"\"\"\n",
    "        \n",
    "        if writer is not None:\n",
    "            with writer.as_default():\n",
    "                tf.summary.histogram(self.name + '_beta', self._beta, step=step)\n",
    "                tf.summary.histogram(self.name + '_gamma', self._gamma, step=step)\n",
    "                tf.summary.histogram(self.name + '_normed', normed, step=step)\n",
    "                tf.summary.histogram(self.name + '_out', out, step=step)\n",
    "                tf.summary.histogram(self.name + '_sigma', sigma, step=step)\n",
    "                tf.summary.histogram(self.name + '_mu', mu, step=step)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b Исправленный батч-норм \n",
    "\n",
    "Запустите эксперимент ниже. \n",
    "\n",
    "Обучается ли сеть? Идет ли процесс обучения лучше, чем в эксперименте с ReLU? \n",
    "\n",
    "Сравните обучение сетей c ReLU и слоем `Dense` (а не `DenseSmart`!) и ReLU с BatchNorm в tensorboard, как в задании 2.\n",
    "Напишите ваши выводы.\n",
    "\n",
    "_Обратите внимание, что слева в интерфейсе tensorboard есть меню, которое позволяет выключать визуализацию ненужных экспериментов._\n",
    "\n",
    "**Ваш комментарий:**\n",
    "\n",
    "<span style=\"color:red \">__Судя по всему, проблема была в том, что sigma становится близкой к 0, или 0, из за этого уходим с ошибкой деления на ноль. Странно только, что не вываливается ексепшен.__</span>\n",
    "\n",
    "<img src=\"img/broken_batchnorm.png\" style=\"width:70%\">\n",
    "\n",
    "<span style=\"color:red \">__Сесть с BatchNorm обучается лучше, чем без него. Видимо, он помогает)__</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 8.798 Train Accuracy: 0.428\n",
      "Epoch 1: Val loss: 5.953 Val Accuracy: 0.595\n",
      "********************\n",
      "Epoch 2: Train loss: 2.866 Train Accuracy: 0.668\n",
      "Epoch 2: Val loss: 0.695 Val Accuracy: 0.784\n",
      "********************\n",
      "Epoch 3: Train loss: 0.597 Train Accuracy: 0.816\n",
      "Epoch 3: Val loss: 0.504 Val Accuracy: 0.845\n",
      "********************\n",
      "Epoch 4: Train loss: 0.467 Train Accuracy: 0.858\n",
      "Epoch 4: Val loss: 0.419 Val Accuracy: 0.875\n",
      "********************\n",
      "Epoch 5: Train loss: 0.395 Train Accuracy: 0.880\n",
      "Epoch 5: Val loss: 0.367 Val Accuracy: 0.892\n",
      "********************\n",
      "Epoch 6: Train loss: 0.345 Train Accuracy: 0.896\n",
      "Epoch 6: Val loss: 0.329 Val Accuracy: 0.902\n",
      "********************\n",
      "Epoch 7: Train loss: 0.307 Train Accuracy: 0.907\n",
      "Epoch 7: Val loss: 0.298 Val Accuracy: 0.913\n",
      "********************\n",
      "Epoch 8: Train loss: 0.275 Train Accuracy: 0.917\n",
      "Epoch 8: Val loss: 0.273 Val Accuracy: 0.920\n",
      "********************\n",
      "Epoch 9: Train loss: 0.246 Train Accuracy: 0.926\n",
      "Epoch 9: Val loss: 0.251 Val Accuracy: 0.927\n",
      "********************\n",
      "Epoch 10: Train loss: 0.221 Train Accuracy: 0.933\n",
      "Epoch 10: Val loss: 0.232 Val Accuracy: 0.932\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "writer = tf.summary.create_file_writer(\"logs/relu_bn_fixed\")\n",
    "\n",
    "model = Sequential(Dense(784, 100, tf.nn.relu, 'dense'), \n",
    "                   BatchNormFixed('batch_norm'), \n",
    "                   Dense(100, 100, tf.nn.relu, 'dense1'), \n",
    "                   Dense(100, 10, tf.nn.softmax, 'dense2'))\n",
    "\n",
    "hist = model.fit_generator(train_seq, test_seq, 10,\n",
    "                           keras.losses.sparse_categorical_crossentropy, \n",
    "                           keras.optimizers.Adam(),\n",
    "                           writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. \"Сырые\" данные. \n",
    "\n",
    "Что будет, если заставить сеть обучаться на сырых данных? \n",
    "\n",
    "Напишите такую функцию `preprocess`, которая не делает min-max scaling изображений и оставляет их в изначальном диапазоне. Не убирайте reshape! Конечно, она должна менять форму матрицы входных данных от `(n x 28 x 28)` к `(n x 784)`. \n",
    "\n",
    "Затем передайте функцию в MNISTSequence, создайте новую train- и test- последовательности запустите эксперимент, используя их как входные данные. \n",
    "\n",
    "Сравните результаты экспериментов c `DenseSmart` + ReLU и обработанными изображениями и `DenseSmart` + ReLU c необработанными изображениями. \n",
    "\n",
    "Обучается ли нейросеть? Если нет, то почему? Сделайте выводы, как в задании 2.\n",
    "\n",
    "**Ваш комментарий:**\n",
    "\n",
    "<span style=\"color:red \">__Нейронная сеть легко может проделать scaling самостоятельно путём установки нужных весов в первом слое. Однако, это потребует множества шагов в связи с тем, что веса нейронной сети инициализируются небольшими значениями, и шаг обучения тоже обычно весьма невилик, чтобы не перепрыгнуть удачный оптимум. По этой причине нейронная сетья будет хуже сходится в случае, если для исходных признаков не выполнен minmax scaling__</span>\n",
    "\n",
    "<img src=\"img/val.png\" style=\"width:70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Шаблон Preprocess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X, y):\n",
    "    return X.reshape((-1, 28*28)), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создание генераторов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_raw = MNISTSequence(X_tr, y_tr, 128, preprocess=preprocess)\n",
    "test_seq_raw = MNISTSequence(X_test, y_test, 128, preprocess=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Эксперимент**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 13.083 Train Accuracy: 0.188\n",
      "Epoch 1: Val loss: 12.774 Val Accuracy: 0.207\n",
      "********************\n",
      "Epoch 2: Train loss: 12.778 Train Accuracy: 0.207\n",
      "Epoch 2: Val loss: 12.696 Val Accuracy: 0.212\n",
      "********************\n",
      "Epoch 3: Train loss: 12.511 Train Accuracy: 0.224\n",
      "Epoch 3: Val loss: 12.651 Val Accuracy: 0.215\n",
      "********************\n",
      "Epoch 4: Train loss: 12.391 Train Accuracy: 0.231\n",
      "Epoch 4: Val loss: 11.516 Val Accuracy: 0.285\n",
      "********************\n",
      "Epoch 5: Train loss: 11.795 Train Accuracy: 0.268\n",
      "Epoch 5: Val loss: 11.426 Val Accuracy: 0.291\n",
      "********************\n",
      "Epoch 6: Train loss: 11.739 Train Accuracy: 0.272\n",
      "Epoch 6: Val loss: 11.307 Val Accuracy: 0.299\n",
      "********************\n",
      "Epoch 7: Train loss: 11.421 Train Accuracy: 0.291\n",
      "Epoch 7: Val loss: 11.463 Val Accuracy: 0.289\n",
      "********************\n",
      "Epoch 8: Train loss: 11.679 Train Accuracy: 0.275\n",
      "Epoch 8: Val loss: 11.600 Val Accuracy: 0.280\n",
      "********************\n",
      "Epoch 9: Train loss: 11.668 Train Accuracy: 0.276\n",
      "Epoch 9: Val loss: 11.699 Val Accuracy: 0.274\n",
      "********************\n",
      "Epoch 10: Train loss: 11.442 Train Accuracy: 0.290\n",
      "Epoch 10: Val loss: 11.312 Val Accuracy: 0.298\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "writer = tf.summary.create_file_writer(\"logs/raw\")\n",
    "\n",
    "model = Sequential(DenseSmart(784, 100, tf.nn.relu, 'dense'), \n",
    "                   DenseSmart(100, 100, tf.nn.relu, 'dense1'), \n",
    "                   DenseSmart(100, 10, tf.nn.softmax, 'dense2'))\n",
    "\n",
    "hist = model.fit_generator(train_seq_raw, test_seq_raw, 10,\n",
    "                           keras.losses.sparse_categorical_crossentropy, \n",
    "                           keras.optimizers.Adam(),\n",
    "                           writer\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
